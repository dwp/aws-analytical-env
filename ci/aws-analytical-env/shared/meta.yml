meta:
  plan:
    terraform-common-config:
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.terraform_repository))
            tag: ((dataworks.terraform_version))
        params:
          TF_INPUT: false
          TF_CLI_ARGS_apply: -lock-timeout=300s
          TF_CLI_ARGS_plan: -lock-timeout=300s
          TF_VAR_costcode: ((dataworks.costcode))

    terraform-14-common-config:
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.terraform_repository))
            tag: ((dataworks.terraform_version))
        params:
          TF_INPUT: false
          TF_CLI_ARGS_apply: -lock-timeout=300s
          TF_CLI_ARGS_plan: -lock-timeout=300s
          TF_VAR_costcode: ((dataworks.costcode))

    terraform-bootstrap:
      task: terraform-bootstrap
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.docker_awscli_repository))
            version: ((dataworks.docker_awscli_version))
            tag: ((dataworks.docker_awscli_version))
        run:
          path: sh
          args:
            - -exc
            - |
              if [ -f ../previous_success/exit-if-succeeded.sh ]; then
                source ../previous_success/exit-if-succeeded.sh
              fi
              python bootstrap_terraform.py
              sed -i '/^assume_role/ d' terraform/deploy/$DEPLOY_PATH/terraform.tfvars
              cp terraform/deploy/$DEPLOY_PATH/terraform.tf ../terraform-config
              cp terraform/deploy/$DEPLOY_PATH/terraform.tfvars ../terraform-config
          dir: aws-analytical-env
        inputs:
          - name: aws-analytical-env
        outputs:
          - name: terraform-config
      params:
        AWS_REGION: eu-west-2

    terraform-apply:
      task: terraform-apply
      .: (( inject meta.plan.terraform-common-config ))
      config:
        run:
          path: sh
          args:
            - -exc
            - |
              if [ -f ../../../../previous_success/exit-if-succeeded.sh ]; then
                source ../../../../previous_success/exit-if-succeeded.sh
              fi
              apk add -q --update --no-cache python3 && ln -sf python3 /usr/bin/python
              python3 -m ensurepip
              pip3 install -q --no-cache --upgrade pip setuptools boto3
              cp ../../../../terraform-config/terraform.tf .
              cp ../../../../terraform-config/terraform.tfvars .
              terraform workspace show
              terraform init
              export TF_VAR_emr_al2_ami_id=$(cat ../../../../emr-al2-ami/id)
              export TF_VAR_aws_analytical_env_emr_launcher_zip="{base_path = \"../../../../emr-launcher-release\", version = \"$(cat ../../../../emr-launcher-release/version)\"}"
              export TF_VAR_manage_mysql_user_lambda_zip="{base_path = \"../../../../manage-mysql-user-release\", version = \"$(cat ../../../../manage-mysql-user-release/version)\"}"
              custom_auth_jar_version=$(cat ../../../../hive-custom-auth-release/version) || true
              export TF_VAR_hive_custom_auth_jar_path="../../../../hive-custom-auth-release/hive-custom-auth-${custom_auth_jar_version}.jar"
              terraform plan -out terraform.plan
              terraform apply -auto-approve terraform.plan
        inputs:
          - name: aws-analytical-env
          - name: terraform-config
          - name: emr-al2-ami

    terraform-plan:
      task: terraform-plan
      .: (( inject meta.plan.terraform-common-config ))
      config:
        run:
          path: sh
          args:
            - -exc
            - |
              if [ -f ../../../../previous_success/exit-if-succeeded.sh ]; then
                source ../../../../previous_success/exit-if-succeeded.sh
              fi
              apk add -q --update --no-cache python3 && ln -sf python3 /usr/bin/python
              python3 -m ensurepip
              pip3 install -q --no-cache --upgrade pip setuptools boto3
              cp ../../../../terraform-config/terraform.tf .
              cp ../../../../terraform-config/terraform.tfvars .
              terraform workspace show
              terraform init
              export TF_VAR_emr_al2_ami_id=$(cat ../../../../emr-al2-ami/id)
              export TF_VAR_aws_analytical_env_emr_launcher_zip="{base_path = \"../../../../emr-launcher-release\", version = \"$(cat ../../../../emr-launcher-release/version)\"}"
              export TF_VAR_manage_mysql_user_lambda_zip="{base_path = \"../../../../manage-mysql-user-release\", version = \"$(cat ../../../../manage-mysql-user-release/version)\"}"
              custom_auth_jar_version=$(cat ../../../../hive-custom-auth-release/version) || true
              export TF_VAR_hive_custom_auth_jar_path="../../../../hive-custom-auth-release/hive-custom-auth-${custom_auth_jar_version}.jar"
              terraform plan
        inputs:
          - name: aws-analytical-env
          - name: terraform-config
          - name: emr-al2-ami

    terraform-output-app:
      task: terraform-output-app
      .: (( inject meta.plan.terraform-common-config ))
      config:
        run:
          path: sh
          args:
            - -exc
            - |
              cp ../../../../terraform-config/terraform.tf .
              cp ../../../../terraform-config/terraform.tfvars .
              terraform workspace show
              terraform init
              export TF_VAR_emr_al2_ami_id=$(cat ../../../../emr-al2-ami/id)
              export TF_VAR_aws_analytical_env_emr_launcher_zip="{base_path = \"../../../../emr-launcher-release\", version = \"$(cat ../../../../emr-launcher-release/version)\"}"
              terraform output --json > ../../../../terraform-output-app/outputs.json
          dir: aws-analytical-env/terraform/deploy/app
        inputs:
          - name: aws-analytical-env
          - name: terraform-config
          - name: emr-al2-ami
        outputs:
          - name: terraform-output-app


    terraform-output-testing:
      task: terraform-output-testing
      .: (( inject meta.plan.terraform-common-config ))
      config:
        run:
          path: sh
          args:
            - -exc
            - |
              if [ -f ../../../../previous_success/exit-if-succeeded.sh ]; then
                source ../../../../previous_success/exit-if-succeeded.sh
              fi
              cp ../../../../terraform-config/terraform.tf .
              cp ../../../../terraform-config/terraform.tfvars .
              terraform workspace show
              terraform init
              terraform output --json > ../../../../terraform-output-testing/outputs.json
              cat ../../../../terraform-output-testing/outputs.json
          dir: aws-analytical-env/terraform/deploy/testing
        inputs:
          - name: aws-analytical-env
          - name: terraform-config
        outputs:
          - name: terraform-output-testing

    mirror-git-repo:
      task: mirror-git-repo
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.docker_awscli_repository))
            version: ((dataworks.docker_awscli_version))
            tag: ((dataworks.docker_awscli_version))
        params:
          AWS_DEFAULT_REGION: ((dataworks.aws_region))
        run:
          path: sh
          args:
            - -exc
            - |
              source /assume-role
              AWS_COMMOM_BUCKET="$(cat ../terraform-output-common/outputs.json |  jq -r '.config_bucket.value.id')"
              aws s3 sync . s3://$AWS_COMMOM_BUCKET/$S3_ROOT --include "*" --exclude ".git/*"
        inputs:
          - name: terraform-output-common

    terraform-output-common:
      task: terraform-output-common
      .: (( inject meta.plan.terraform-14-common-config ))
      config:
        run:
          path: sh
          args:
            - -exc
            - |
              terraform workspace show
              terraform init
              terraform output --json > ../terraform-output-common/outputs.json
          dir: aws-common-infrastructure
        inputs:
          - name: aws-common-infrastructure
        outputs:
          - name: terraform-output-common

    terraform-bootstrap-crown:
      task: terraform-bootstrap-crown
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.docker_awscli_repository))
            version: ((dataworks.docker_awscli_version))
            tag: ((dataworks.docker_awscli_version))
        run:
          path: sh
          args:
            - -exc
            - |
              python bootstrap_terraform.py
              sed -i '/^assume_role/ d' terraform.tfvars
              cp terraform.tf ../terraform-config-crown
              cp terraform.tfvars ../terraform-config-crown
          dir: aws-analytical-dataset-generation
        inputs:
          - name: aws-analytical-dataset-generation
        outputs:
          - name: terraform-config-crown
      params:
        AWS_ACCESS_KEY_ID: ((ci.aws_access_key_id))
        AWS_SECRET_ACCESS_KEY: ((ci.aws_secret_access_key))
        AWS_REGION: eu-west-2

    create-aws-profiles:
      task: create-aws-profiles
      attempts: 15
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((docker-awscli-ubuntu.repository))
            version: ((docker-awscli-ubuntu.version))
            tag: ((docker-awscli-ubuntu.version))
        outputs:
          - name: .aws
        params:
          AWS_ACCESS_KEY_ID: ((ci.aws_access_key_id))
          AWS_SECRET_ACCESS_KEY: ((ci.aws_secret_access_key))
        run:
          path: sh
          args:
            - -exc
            - |
              cat <<EOF> .aws/credentials
              [default]
              region = eu-west-1
              aws_access_key_id = $AWS_ACCESS_KEY_ID
              aws_secret_access_key = $AWS_SECRET_ACCESS_KEY

              [ci]
              region = eu-west-1
              role_arn = $AWS_ROLE
              source_profile = default
              s3 =
                max_concurrent_requests = 5
                signature_version = s3v4
              EOF

    create-crown-credentials:
      task: create-crown-credentials
      attempts: 15
      input_mapping:
        git-repo: aws-analytical-env
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((docker-awscli-ubuntu.repository))
            version: ((docker-awscli-ubuntu.version))
            tag: ((docker-awscli-ubuntu.version))
        inputs:
          - name: git-repo
        outputs:
          - name: ssh-credentials
        params:
          PRIVATE_KEY: ((crown.private_key))
        run:
          path: sh
          dir: ssh-credentials
          args:
            - -exc
            - |
              cat <<EOF> id_rsa
              $PRIVATE_KEY
              EOF
              mv ../git-repo/ci/aws-analytical-env/templates/ssh_config.tpl ./config
              chmod 400 id_rsa

    terraform-output-adg:
      task: terraform-output-adg
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((terraform.repository))
            version: ((terraform12.version))
            tag: ((terraform12.version))
        run:
          path: sh
          args:
            - -exc
            - |
              cp ../terraform-config-crown/terraform.tf .
              cp ../terraform-config-crown/terraform.tfvars .
              terraform workspace show
              terraform init
              terraform output --json > ../terraform-output-adg/outputs.json
          dir: aws-analytical-dataset-generation
        inputs:
          - name: aws-analytical-dataset-generation
          - name: terraform-config-crown
        outputs:
          - name: terraform-output-adg
        params:
          TF_CLI_ARGS_plan: -lock-timeout=300s
          TF_INPUT: "false"
          AWS_ACCESS_KEY_ID: ((ci.aws_access_key_id))
          AWS_SECRET_ACCESS_KEY: ((ci.aws_secret_access_key))

    terraform-output-common-crown:
      task: terraform-output-common-crown
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((terraform.repository))
            version: 0.14.9
            tag: 0.14.9
        run:
          path: sh
          args:
            - -exc
            - |
              pwd
              terraform workspace show
              terraform init
              terraform output --json > ../terraform-output-common-crown/outputs.json
          dir: aws-common-infrastructure
        inputs:
          - name: aws-common-infrastructure
        outputs:
          - name: terraform-output-common-crown
        params:
          TF_CLI_ARGS_plan: -lock-timeout=300s
          TF_INPUT: "false"
          AWS_ACCESS_KEY_ID: ((ci.aws_access_key_id))
          AWS_SECRET_ACCESS_KEY: ((ci.aws_secret_access_key))

    get-files:
      task: get-files
      attempts: 50
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((docker-awscli-ubuntu.repository))
            version: ((docker-awscli-ubuntu.version))
            tag: ((docker-awscli-ubuntu.version))
        inputs:
          - name: ssh-credentials
        outputs:
          - name: files
        params:
          RETRY_SLEEP_TIME: 600
          TARGET_SERVER: "hdp-edge01.node.prd.dw"
        run:
          path: sh
          args:
            - -exc
            - |
              whoami
              cp -R ssh-credentials ~/.ssh
              scp -r -o StrictHostKeyChecking=no hdp-edge01.node.prd.dw:${SOURCE_PATH}/* files/

    load-s3:
      task: load-s3
      attempts: 15
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((docker-awscli-ubuntu.repository))
            version: ((docker-awscli-ubuntu.version))
            tag: ((docker-awscli-ubuntu.version))
        inputs:
          - name: files
          - name: .aws
          - name: terraform-output-common-crown
        params:
          AWS_PROFILE: ci
          AWS_SHARED_CREDENTIALS_FILE: .aws/credentials
        run:
          path: sh
          args:
            - -exc
            - |
              AWS_PUBLISHED_BUCKET="$(cat terraform-output-common-crown/outputs.json |  jq -r '.published_bucket.value.id')"
              aws s3 --endpoint-url=https://s3-eu-west-1.amazonaws.com sync files/ s3://${AWS_PUBLISHED_BUCKET}/${S3_FOLDER}

    get-cyi-files:
      task: get-cyi-files
      attempts: 50
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((docker-awscli-ubuntu.repository))
            version: ((docker-awscli-ubuntu.version))
            tag: ((docker-awscli-ubuntu.version))
        inputs:
          - name: ssh-credentials
        outputs:
          - name: cyi-files
        params:
          RETRY_SLEEP_TIME: 600
          TARGET_SERVER: "hdp-edge01.node.prd.dw"
        run:
          path: sh
          args:
            - -exc
            - |
              whoami
              cp -R ssh-credentials ~/.ssh
              scp -r -o StrictHostKeyChecking=no hdp-edge01.node.prd.dw:${SOURCE_PATH}/* cyi-files/

    load-cyi-s3:
        task: load-cyi-s3
        attempts: 15
        config:
          platform: linux
          image_resource:
            type: docker-image
            source:
              repository: ((docker-awscli-ubuntu.repository))
              version: ((docker-awscli-ubuntu.version))
              tag: ((docker-awscli-ubuntu.version))
          inputs:
            - name: cyi-files
            - name: .aws
            - name: terraform-output-common-crown
          params:
            AWS_PROFILE: ci
            AWS_SHARED_CREDENTIALS_FILE: .aws/credentials
          run:
            path: sh
            args:
              - -exc
              - |
                # use 25... ingestion-input-storage bucketID
                AWS_PUBLISHED_BUCKET="SOME-BUCKET-ID"
                aws s3 --endpoint-url=https://s3-eu-west-1.amazonaws.com sync cyi-files/ s3://${AWS_PUBLISHED_BUCKET}/${S3_FOLDER}

    get-cyi-files-from-s3:
      task: get-cyi-files-from-s3
      attempts: 10
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((docker-awscli-ubuntu.repository))
            version: ((docker-awscli-ubuntu.version))
            tag: ((docker-awscli-ubuntu.version))
        inputs:
          - name: .aws
        outputs:
          - name: cyi-files-from-s3
        params:
          AWS_PROFILE: ci
          AWS_SHARED_CREDENTIALS_FILE: .aws/credentials
        run:
          path: sh
          args:
            - -exc
            - |
              YESTERDAY="$(date +%Y-%m-%d -d 'yesterday')"
              # use 25... ingestion-input-storage bucketID
              SOME-BUCKET-ID="XXXXX"
              aws s3 --endpoint-url=https://s3-eu-west-1.amazonaws.com cp --recursive s3://${SOME-BUCKET-ID}/${S3_FOLDER}/${YESTERDAY} cyi-files-from-s3/.
              chmod 766 -R cyi-files-from-s3/

    copy-to-crown-cyi-files:
      task: copy-to-crown-cyi-files
      attempts: 10
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((docker-awscli-ubuntu.repository))
            version: ((docker-awscli-ubuntu.version))
            tag: ((docker-awscli-ubuntu.version))
        inputs:
          - name: cyi-files-from-s3
          - name: ssh-credentials
        params:
          RETRY_SLEEP_TIME: 600
          TARGET_SERVER: "hdp-edge01.node.prd.dw"
        run:
          path: sh
          args:
            - -exc
            - |
              whoami
              cp -R ssh-credentials ~/.ssh
              scp -r -o StrictHostKeyChecking=no cyi-files-from-s3/* hdp-edge01.node.prd.dw:${TARGET_PATH}/.
              ssh -o StrictHostKeyChecking=no hdp-edge01.node.prd.dw "sudo su -l awsingest /cyi-historical-data/from-s3/copy-cyi-to-hdfs.sh"

    stop-waiting-cluster:
      task: stop-waiting-cluster
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.docker_awscli_repository))
            version: ((dataworks.docker_awscli_version))
            tag: ((dataworks.docker_awscli_version))
        params:
          AWS_ROLE_ARN: arn:aws:iam::((aws_account.management)):role/ci
          AWS_DEFAULT_REGION: ((dataworks.aws_region))
        run:
          path: sh
          args:
            - -exc
            - |
              export AWS_DEFAULT_REGION
              source /assume-role
              set +x

              for CLUSTER_ID in `aws emr list-clusters --cluster-state WAITING | jq -r '.Clusters[].Id'`
              do
                  echo "Checking cluster $CLUSTER_ID"

                  # Is it an analytical cluster (should probably tag type of analytical-env ).

                  APPNAME=`aws emr describe-cluster --cluster-id $CLUSTER_ID | jq -r ".Cluster.Tags[] | if .Key == \"Application\" then .Value else empty end"`
                  CLEVEL=`aws emr describe-cluster --cluster-id $CLUSTER_ID | jq -r ".Cluster.StepConcurrencyLevel"`

                  if [[ ${APPNAME:-""} == "aws-analytical-env" ]]
                  then
                      echo $CLUSTER_ID is an analytical env cluster

                      # Currently the only way to know if its a batch cluster is to check if its concurrency is 256

                      if [[ $CLEVEL -eq 256 ]]
                      then
                          echo $CLUSTER_ID is a batch cluster
                          END_DATE=`aws emr list-steps --cluster-id $CLUSTER_ID | jq '.Steps[0].Status.Timeline.EndDateTime | floor'`
                          let DELTA="`date +%s` - $END_DATE"

                          if [[ $DELTA -gt 3600 ]]
                          then
                              echo Stopping cluster $CLUSTER_ID with a delta of $DELTA
                              aws emr terminate-clusters --cluster-ids $CLUSTER_ID
                          fi
                      fi
                  fi
              done

    mirror-ucsqlhelper:
      task: mirror-ucsqlhelper
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.docker_awscli_repository))
            version: ((dataworks.docker_awscli_version))
            tag: ((dataworks.docker_awscli_version))
        params:
          AWS_DEFAULT_REGION: ((dataworks.aws_region))
          MIRROR_S3_KEY_PREFIX: component/aws-analytical-env/r-packages
        run:
          path: sh
          args:
            - -exc
            - |
              source /assume-role
              tar -zcf ucsqlhelper.tar.gz ucsqlhelper
              AWS_COMMOM_BUCKET="$(cat ./terraform-output-common/outputs.json |  jq -r '.config_bucket.value.id')"
              aws s3 cp ucsqlhelper.tar.gz s3://$AWS_COMMOM_BUCKET/$MIRROR_S3_KEY_PREFIX/ucsqlhelper.tar.gz
        inputs:
          - name: ucsqlhelper
          - name: terraform-output-common


    test-emr-cluster:
      task: test-emr-cluster
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.docker_awscli_repository))
            version: ((dataworks.docker_awscli_version))
            tag: ((dataworks.docker_awscli_version))
        params:
          AWS_DEFAULT_REGION: ((dataworks.aws_region))
        run:
          path: sh
          args:
            - -exc
            - |
              if [ -f ./previous_success/exit-if-succeeded.sh ]; then
                source ./previous_success/exit-if-succeeded.sh
              fi
              source /assume-role
              LAMBDA_NAME="$(jq -r '.testing.value.test_lambda.function_name' < ./terraform-output-testing/outputs.json)"
              sleep 300 # wait for cluster to finish provisioning
              aws lambda invoke --function-name $LAMBDA_NAME --invocation-type RequestResponse --cli-connect-timeout 900 --cli-read-timeout 900 output.json
              jq -eC "if .errorMessage? then error(.errorMessage) else true end" < output.json
        inputs:
          - name: terraform-output-testing

    ami-test-results:
      task: ami-test-result
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.terraform_repository))
            tag: ((dataworks.terraform_version))
        params:
          AWS_DEFAULT_REGION: ((dataworks.aws_region))
          GIT_USERNAME: ((dataworks.concourse_github_username))
          GIT_EMAIL: ((dataworks.concourse_github_email))
          GITHUB_TOKEN: ((dataworks-secrets.concourse_github_pat))
        run:
          path: sh
          args:
            - -exc
            - |
              set +x
              source ./previous_success/exit-if-succeeded.sh

              AMI_ID=$(cat emr-al2-ami/id)
              DATE=$(date -u)
              PIPELINE="$(cat meta/build_pipeline_name)"

              git config --global user.name "${GIT_USERNAME}"
              git config --global user.email "${GIT_EMAIL}"

              git clone https://${GITHUB_TOKEN}:x-oauth-basic@github.com/dwp/ami-builder-configs
              cd ami-builder-configs/results

              echo "$AMI_ID $RESULT" > "$PIPELINE.test"

              git add "$PIPELINE.test"
              git commit -m "Updating $PIPELINE AMI test on ${DATE}"
              git push https://${GITHUB_TOKEN}:x-oauth-basic@github.com/dwp/ami-builder-configs
        inputs:
          - name: meta
          - name: emr-al2-ami
          - name: previous_success

    check-ami-test-results:
      task: check-ami-test-result
      config:
        platform: linux
        image_resource:
          type: docker-image
          source:
            repository: ((dataworks.terraform_repository))
            tag: ((dataworks.terraform_version))
        params:
          AWS_DEFAULT_REGION: ((dataworks.aws_region))
          GIT_USERNAME: ((dataworks.concourse_github_username))
          GIT_EMAIL: ((dataworks.concourse_github_email))
          GITHUB_TOKEN: ((dataworks-secrets.concourse_github_pat))
        run:
          path: sh
          args:
            - -exc
            - |
              set +x
              PREVIOUS_SUCCESS=false
              AMI_ID=$(cat untested_ami/id)
              DATE=$(date -u)
              PIPELINE="$(cat meta/build_pipeline_name)"
              PATH_TO_RESULTS="ami-builder-configs/results"

              git config --global user.name "${GIT_USERNAME}"
              git config --global user.email "${GIT_EMAIL}"
              git clone https://${GITHUB_TOKEN}:x-oauth-basic@github.com/dwp/ami-builder-configs

              if [ -f "./$PATH_TO_RESULTS/$PIPELINE.test" ]; then
                set +e
                grep "$AMI_ID SUCCESS" "./$PATH_TO_RESULTS/$PIPELINE.test"
                if [ $? -eq 0 ]; then
                  PREVIOUS_SUCCESS=true
                fi
                set -e
              fi

              touch ./previous_success/exit-if-succeeded.sh

              if $PREVIOUS_SUCCESS; then
                 echo 'echo "AMI already passed. Exiting..."; exit 0' > ./previous_success/exit-if-succeeded.sh
              fi

              chmod +x ./previous_success/exit-if-succeeded.sh
        outputs:
          - name: previous_success
        inputs:
          - name: meta
          - name: untested_ami
